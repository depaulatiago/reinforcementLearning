{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/depaulatiago/reinforcementLearning/blob/main/atvVivencial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resolução de Problemas de Otimização sob Incerteza em Cadeias de Suprimentos através de Técnicas de Aprendizado por Reforço**\n",
        "###**Atividade Vivencial**\n",
        "### **Membros do Projeto**\n",
        "* Julio César Alves\n",
        "* Tiago de Paula Martins"
      ],
      "metadata": {
        "id": "EwXN4gCTTmfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Capítulo 1 do livro Grokking Deep Reinforcement Learning**\n",
        "\n",
        " **O que é Reep Reinforcement Rearning**\n",
        "\n",
        "DRL é uma abordagem de aprendizado de máquina que se concentra na criação de programas de computador capazes\n",
        "de resolver problemas que exigem inteligência.\n",
        "O que torna os programas de DRL distintos é sua capacidade de aprender por tentativa e erro, usando feedback\n",
        "que é sequencial, avaliativo e amostrado ao mesmo tempo.\n",
        "Sendo um dos ramos de machine learning para uma inteligência artificial.\n",
        "\n",
        "---\n",
        "\n",
        "**Principais ramos de Machine Learning**\n",
        "\n",
        "* Supervised Learning (SL) - envolve aprender com dados rotulados, onde um humano decide quais dados coletar e como rotulá-los.\n",
        "* Unsupervised Learning (UL) - o aprendizado é feito com dados não rotulados, com métodos de coleta projetados por humanos.\n",
        "* Reinforcement Learning (RL) - não há rotulação de dados nem projeto explícito de coleta de dados por humanos, aprende por meio de tentativa e erro\n",
        "\n",
        "---\n",
        "\n",
        "**Fatores de Reinforced Learning**\n",
        "\n",
        "* Ambiente: engloba tudo o que está fora do controle direto do agente.Ao treinar um robô para pegar objetos, os objetos, a bandeja, o vento e outros elementos externos ao tomador de decisão são considerados parte do ambiente.\n",
        "\n",
        "* Agente: seu processo envolve três etapas: interação com o ambiente, avaliação do comportamento e melhoria das respostas. O agente pode aprender mapeamentos de observações para ações, chamados policies. Ele também pode aprender o modelo do ambiente em mapeamentos, denominados models. Além disso, pode ser projetado para estimar a recompensa em mapeamentos chamados value functions.\n",
        "\n",
        "* Recompensa: é o feedback que o agente recebe após realizar uma ação, ajudando a guiar seu comportamento. Pode ser imediata ou atrasada, e o agente deve aprender a atribuir corretamente o crédito às ações passadas. Isso é desafiador, especialmente com recompensas escassas ou fracas. A capacidade de aprender com feedback amostrado e generalizar é crucial para o sucesso do agente em ambientes complexos.\n",
        "\n",
        "---\n",
        "\n",
        "**Como agentes aprendem**\n",
        "\n",
        "* Sequential Feedback: A ação do agente pode resultar em consequências futuras, com recompensas ocorrendo após vários passos. Portanto, o agente precisa aprender com sequential feedback, levando ao temporal credit assignment problem. Esse desafio consiste em determinar quais estados e/ou ações contribuíram para uma recompensa, especialmente quando as ações têm efeitos retardados, tornando difícil atribuir crédito às recompensas.\n",
        "\n",
        "* Evaluative Feedback: A recompensa recebida pelo agente pode ser fraca, pois pode não fornecer supervisão e apenas indicar a qualidade, não necessariamente a correção da ação, podendo não conter informações sobre outras possíveis recompensas. Portanto, o agente deve ser capaz de aprender com evaluative feedback, o que demanda a necessidade de exploração. O agente deve equilibrar a coleta de informações com a exploração de informações atuais, em um compromisso entre exploração e exploração.\n",
        "\n",
        "* Sampled Feedback: A recompensa recebida pelo agente é apenas uma amostra, e o agente não tem acesso à função de recompensa. Além disso, os espaços de estado e ação são frequentemente grandes, até infinitos, tornando difícil aprender com feedback escasso e fraco utilizando amostras. Portanto, o agente precisa ser capaz de aprender a partir de sampled feedback e deve ser capaz de generalizar para lidar com essa dificuldade.\n",
        "\n",
        "* Powerful Non-linear Function Approximation: O agente pode aproximar funções usando uma variedade de métodos e técnicas de ML, desde árvores de decisão até SVMs e redes neurais.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hTKLTJoLbzes"
      }
    }
  ]
}